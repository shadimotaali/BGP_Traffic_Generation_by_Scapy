{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bfaf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Configuration variables for Jupyter notebook\n",
    "INPUT_FILE = \"/Users/shadimoteali/PhD/BGP_Traffic_Generation/results/bgp_updates_analysis.csv\"\n",
    "OUTPUT_FILE = \"/Users/shadimoteali/PhD/BGP_Traffic_Generation/results/bgp_features_1min.csv\"\n",
    "WINDOW_SIZE = '5s'  # You can change to '30s', '5min', etc.\n",
    "LABEL_STRATEGY = 'majority'  # Options: 'majority', 'conservative', 'weighted'\n",
    "\n",
    "def calculate_edit_distance(as_path1, as_path2):\n",
    "    \"\"\"\n",
    "    Calculate edit distance between two AS paths\n",
    "    \"\"\"\n",
    "    if not as_path1 or not as_path2:\n",
    "        return 0\n",
    "    \n",
    "    # Handle integer AS paths by converting them to single-item lists\n",
    "    if isinstance(as_path1, int):\n",
    "        as_path1 = [as_path1]\n",
    "    \n",
    "    if isinstance(as_path2, int):\n",
    "        as_path2 = [as_path2]\n",
    "    \n",
    "    # Convert paths to lists if they are strings\n",
    "    if isinstance(as_path1, str):\n",
    "        # Handle cases with special characters\n",
    "        as_path1 = as_path1.replace('{', '').replace('}', '')\n",
    "        as_path1 = [int(as_num) for as_num in as_path1.split() if as_num.isdigit()]\n",
    "    \n",
    "    if isinstance(as_path2, str):\n",
    "        # Handle cases with special characters\n",
    "        as_path2 = as_path2.replace('{', '').replace('}', '')\n",
    "        as_path2 = [int(as_num) for as_num in as_path2.split() if as_num.isdigit()]\n",
    "    \n",
    "    if not as_path1 or not as_path2:\n",
    "        return 0\n",
    "    \n",
    "    m, n = len(as_path1), len(as_path2)\n",
    "    \n",
    "    # Initialize the distance matrix\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    # Fill the first row and column\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    # Calculate edit distance\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if as_path1[i-1] == as_path2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    "    \n",
    "    return dp[m][n]\n",
    "    \n",
    "\n",
    "def extract_features(df_window):\n",
    "    \"\"\"\n",
    "    Extract BGP features from a dataframe within a specific time window\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "\n",
    "    # 1. ANNOUNCEMENTS\n",
    "    # Count number of updates by type\n",
    "    announcements = df_window[df_window['Subtype'] == 'ANNOUNCE']\n",
    "    \n",
    "    # 2. WITHDRAWALS - including all withdrawal types\n",
    "    withdrawal_types = ['WITHDRAW', 'WITHDRAW_MP_UNREACH_NLRI_AFI2']\n",
    "    withdrawals = df_window[df_window['Subtype'].isin(withdrawal_types)]\n",
    "    \n",
    "    # Store counts\n",
    "    features['announcements'] = len(announcements)\n",
    "    features['withdrawals'] = len(withdrawals)\n",
    "    \n",
    "    # 3. NLRI_ANN (Network Layer Reachability Information announcements)\n",
    "    features['nlri_ann'] = features['announcements']\n",
    "    \n",
    "    # 4. DUPLICATES\n",
    "    # Proper implementation for duplicates: same prefix announced repeatedly with the same attributes\n",
    "    if not announcements.empty:\n",
    "        # Group by all relevant attributes and count instances > 1\n",
    "        dup_cols = ['Peer_IP', 'Peer_ASN', 'Prefix', 'AS_Path', 'Origin', 'Next_Hop', 'MED', 'Local_Pref', 'Communities']\n",
    "        dup_cols = [col for col in dup_cols if col in announcements.columns]  # Only include columns that exist\n",
    "        \n",
    "        # Count announcements for same prefix/path/attributes\n",
    "        announcement_counts = announcements.groupby(dup_cols).size()\n",
    "        duplicates = sum(count - 1 for count in announcement_counts if count > 1)\n",
    "        features['dups'] = duplicates\n",
    "    else:\n",
    "        features['dups'] = 0\n",
    "    \n",
    "    # 5-7. ORIGIN ATTRIBUTES\n",
    "    # Only count origin attributes that are present in your data\n",
    "    if not announcements.empty and 'Origin' in announcements.columns:\n",
    "        origin_counts = announcements['Origin'].value_counts()\n",
    "        features['origin_0'] = origin_counts.get('IGP', 0)  \n",
    "      \n",
    "        features['origin_2'] = origin_counts.get('INCOMPLETE', 0)  \n",
    "        \n",
    "        # 8. ORIGIN CHANGES\n",
    "        # Count prefixes with more than one unique origin attribute\n",
    "        if not announcements.empty:\n",
    "            unique_prefix_origins = announcements.groupby('Prefix')['Origin'].nunique()\n",
    "            features['origin_changes'] = (unique_prefix_origins > 1).sum()\n",
    "        else:\n",
    "            features['origin_changes'] = 0\n",
    "    else:\n",
    "        features['origin_0'] = 0\n",
    "        features['origin_2'] = 0\n",
    "        features['origin_changes'] = 0\n",
    "    \n",
    "    # 9-10. AS PATH METRICS\n",
    "    if not announcements.empty and 'AS_Path' in announcements.columns:\n",
    "        # Filter out empty AS paths\n",
    "        valid_as_paths = announcements[announcements['AS_Path'].notna() & (announcements['AS_Path'] != '')]\n",
    "        \n",
    "        if not valid_as_paths.empty:\n",
    "            # Calculate AS path lengths\n",
    "            as_path_lengths = valid_as_paths['AS_Path'].apply(\n",
    "                lambda path: len([p for p in path.split() if p.isdigit()]) if isinstance(path, str) else 0\n",
    "            )\n",
    "            \n",
    "            # 9. AS_PATH_MAX: Maximum AS path length\n",
    "            features['as_path_max'] = as_path_lengths.max() if not as_path_lengths.empty else 0\n",
    "            \n",
    "            # 10. UNIQUE_AS_PATH_MAX: Maximum number of unique AS paths per prefix\n",
    "            unique_paths_per_prefix = valid_as_paths.groupby('Prefix')['AS_Path'].nunique()\n",
    "            features['unique_as_path_max'] = unique_paths_per_prefix.max() if not unique_paths_per_prefix.empty else 0\n",
    "            \n",
    "            # 11-20. EDIT DISTANCE FEATURES\n",
    "            # Calculate edit distances between consecutive AS paths for each prefix\n",
    "            edit_distances = []\n",
    "            edit_distance_dict = defaultdict(list)\n",
    "            \n",
    "            for prefix, group in valid_as_paths.groupby('Prefix'):\n",
    "                if len(group) >= 2:\n",
    "                    sorted_group = group.sort_values('Timestamp')\n",
    "                    prev_path = None\n",
    "                    \n",
    "                    for _, row in sorted_group.iterrows():\n",
    "                        current_path = row['AS_Path']\n",
    "                        \n",
    "                        if prev_path is not None:\n",
    "                            dist = calculate_edit_distance(prev_path, current_path)\n",
    "                            edit_distances.append(dist)\n",
    "                            edit_distance_dict[prefix].append(dist)\n",
    "                        \n",
    "                        prev_path = current_path\n",
    "            \n",
    "            if edit_distances:\n",
    "                # 11. EDIT_DISTANCE_AVG\n",
    "                features['edit_distance_avg'] = np.mean(edit_distances)\n",
    "                \n",
    "                # 12. EDIT_DISTANCE_MAX\n",
    "                features['edit_distance_max'] = max(edit_distances)\n",
    "                \n",
    "                # 13-19. EDIT_DISTANCE_DICT_X (Distribution of edit distances)\n",
    "                edit_dist_counter = Counter(edit_distances)\n",
    "                for i in range(7):  # 0 to 6\n",
    "                    features[f'edit_distance_dict_{i}'] = edit_dist_counter.get(i, 0)\n",
    "                \n",
    "                # 20-21. EDIT_DISTANCE_UNIQUE_DICT_X (Distribution of unique edit distances per prefix)\n",
    "                unique_edit_dists = {}\n",
    "                for prefix, dists in edit_distance_dict.items():\n",
    "                    unique_dists = set(dists)\n",
    "                    for dist in unique_dists:\n",
    "                        if dist in unique_edit_dists:\n",
    "                            unique_edit_dists[dist] += 1\n",
    "                        else:\n",
    "                            unique_edit_dists[dist] = 1\n",
    "                \n",
    "                for i in range(2):  # 0 to 1 for unique distributions\n",
    "                    features[f'edit_distance_unique_dict_{i}'] = unique_edit_dists.get(i, 0)\n",
    "            else:\n",
    "                features['edit_distance_avg'] = 0\n",
    "                features['edit_distance_max'] = 0\n",
    "                for i in range(7):\n",
    "                    features[f'edit_distance_dict_{i}'] = 0\n",
    "                for i in range(2):\n",
    "                    features[f'edit_distance_unique_dict_{i}'] = 0\n",
    "        else:\n",
    "            features['as_path_max'] = 0\n",
    "            features['unique_as_path_max'] = 0\n",
    "            features['edit_distance_avg'] = 0\n",
    "            features['edit_distance_max'] = 0\n",
    "            for i in range(7):\n",
    "                features[f'edit_distance_dict_{i}'] = 0\n",
    "            for i in range(2):\n",
    "                features[f'edit_distance_unique_dict_{i}'] = 0\n",
    "    else:\n",
    "        features['as_path_max'] = 0\n",
    "        features['unique_as_path_max'] = 0\n",
    "        features['edit_distance_avg'] = 0\n",
    "        features['edit_distance_max'] = 0\n",
    "        for i in range(7):\n",
    "            features[f'edit_distance_dict_{i}'] = 0\n",
    "        for i in range(2):\n",
    "            features[f'edit_distance_unique_dict_{i}'] = 0\n",
    "    \n",
    "    # 22-24. IMPLICIT WITHDRAWAL FEATURES\n",
    "    if not announcements.empty:\n",
    "    # Group by prefix and peer to count prefixes with multiple announcements\n",
    "        prefix_peer_groups = announcements.groupby(['Prefix', 'Peer_IP'])\n",
    "    \n",
    "    # Count prefixes with multiple announcements\n",
    "        imp_wd_prefixes = 0\n",
    "        imp_wd_spath_prefixes = 0\n",
    "        imp_wd_dpath_prefixes = 0\n",
    "    \n",
    "        for (prefix, peer), group in prefix_peer_groups:\n",
    "            if len(group) > 1:\n",
    "            # This prefix has implicit withdrawals\n",
    "                imp_wd_prefixes += 1\n",
    "            \n",
    "            # Check if all AS paths are the same or different\n",
    "                if 'AS_Path' in group.columns:\n",
    "                    unique_paths = group['AS_Path'].nunique()\n",
    "                    if unique_paths == 1:\n",
    "                    # All announcements have the same AS path\n",
    "                        imp_wd_spath_prefixes += 1\n",
    "                    else:\n",
    "                    # Announcements have different AS paths\n",
    "                        imp_wd_dpath_prefixes += 1\n",
    "    \n",
    "        features['imp_wd'] = imp_wd_prefixes\n",
    "        features['imp_wd_spath'] = imp_wd_spath_prefixes\n",
    "        features['imp_wd_dpath'] = imp_wd_dpath_prefixes\n",
    "    else:\n",
    "        features['imp_wd'] = 0\n",
    "        features['imp_wd_spath'] = 0\n",
    "        features['imp_wd_dpath'] = 0\n",
    "\n",
    "\n",
    "    # 25-26. RARE AND NEW AS NUMBERS FEATURES\n",
    "    if not announcements.empty and 'AS_Path' in announcements.columns:\n",
    "        all_asns = []\n",
    "    \n",
    "        # Process each AS_Path\n",
    "        for as_path in announcements['AS_Path']:\n",
    "            # Skip null/NaN values\n",
    "            if pd.isnull(as_path) or as_path == '':\n",
    "                continue\n",
    "            \n",
    "            # Convert to string if it isn't already (just to be safe)\n",
    "            as_path_str = str(as_path)\n",
    "        \n",
    "            # Simple case: Single ASN (e.g. \"43289\")\n",
    "            if as_path_str.isdigit():\n",
    "                all_asns.append(as_path_str)\n",
    "                continue\n",
    "            \n",
    "            # Complex case: Path with multiple ASNs\n",
    "            # Remove special characters and split by whitespace\n",
    "            as_path_str = as_path_str.replace('{', '').replace('}', '')\n",
    "            path_asns = [asn for asn in as_path_str.split() if asn.isdigit()]\n",
    "            all_asns.extend(path_asns)\n",
    "    \n",
    "        # Count the occurrence of each ASN\n",
    "        asn_counts = Counter(all_asns)\n",
    "    \n",
    "        # Define rare ASNs (those appearing less than the threshold)\n",
    "        rare_threshold = 3\n",
    "        rare_asns = [asn for asn, count in asn_counts.items() if count < rare_threshold]\n",
    "    \n",
    "        # Calculate features\n",
    "        features['number_rare_ases'] = len(rare_asns)\n",
    "        features['rare_ases_avg'] = len(rare_asns) / len(all_asns) if all_asns else 0\n",
    "    else:\n",
    "        # Default values if no data or AS_Path column\n",
    "        features['number_rare_ases'] = 0\n",
    "        features['rare_ases_avg'] = 0\n",
    "    \n",
    "\n",
    "        \n",
    "    # 27-28. FLAP AND NADA FEATURES\n",
    "    if not df_window.empty:\n",
    "        # FLAPS: A flap is when a prefix is withdrawn and then announced again\n",
    "        # Get all prefixes that were both withdrawn and announced in this window\n",
    "        if not withdrawals.empty and not announcements.empty:\n",
    "            withdrawn_prefixes = set(withdrawals['Prefix'].dropna())\n",
    "            announced_prefixes = set(announcements['Prefix'].dropna())\n",
    "            flapped_prefixes = withdrawn_prefixes.intersection(announced_prefixes)\n",
    "            features['flaps'] = len(flapped_prefixes)\n",
    "        else:\n",
    "            features['flaps'] = 0\n",
    "        \n",
    "        # NADAS (Network Attacks and Defenses Assessment)\n",
    "        # This requires a more complex heuristic to detect potential attack patterns\n",
    "        # For now, we'll implement a basic heuristic looking at suspicious patterns:\n",
    "        # 1. Multiple very specific prefixes (e.g., /32) announced in a short window\n",
    "        # 2. Rapid withdrawal of previously announced prefixes\n",
    "        \n",
    "        # Count very specific prefixes (potential DOS indicators)\n",
    "        very_specific_prefixes = 0\n",
    "        if 'Prefix' in df_window.columns:\n",
    "            # Count /32 prefixes which are typical in some DoS attacks\n",
    "            very_specific_prefixes = sum(1 for prefix in df_window['Prefix'].dropna() \n",
    "                                       if isinstance(prefix, str) and prefix.endswith('/32'))\n",
    "        \n",
    "        # Ratio of withdrawals to announcements\n",
    "        wd_ann_ratio = features['withdrawals'] / features['announcements'] if features['announcements'] > 0 else 0\n",
    "        \n",
    "        # Basic NADAS score - a simple heuristic\n",
    "        nadas_score = very_specific_prefixes + (wd_ann_ratio > 0.5) * 10\n",
    "        features['nadas'] = nadas_score\n",
    "    else:\n",
    "        features['flaps'] = 0\n",
    "        features['nadas'] = 0\n",
    "    \n",
    "    # Determine label based on window data\n",
    "    if 'Label' in df_window.columns:\n",
    "        labels = df_window['Label'].value_counts()\n",
    "        if not labels.empty:\n",
    "            # Choose labeling strategy\n",
    "            if LABEL_STRATEGY == 'majority':\n",
    "                # Majority vote\n",
    "                features['label'] = labels.idxmax()\n",
    "            elif LABEL_STRATEGY == 'conservative':\n",
    "                # If any abnormal, label as abnormal\n",
    "                if any(label != 'normal' for label in labels.index):\n",
    "                    abnormal_labels = [label for label in labels.index if label != 'normal']\n",
    "                    features['label'] = abnormal_labels[0]\n",
    "                else:\n",
    "                    features['label'] = 'normal'\n",
    "            elif LABEL_STRATEGY == 'weighted':\n",
    "                # Weight by count of each label\n",
    "                total = labels.sum()\n",
    "                abnormal_weight = sum(count for label, count in labels.items() if label != 'normal') / total\n",
    "                if abnormal_weight > 0.3:  # Threshold for abnormal classification\n",
    "                    abnormal_labels = [label for label in labels.index if label != 'normal']\n",
    "                    features['label'] = abnormal_labels[0] if abnormal_labels else 'normal'\n",
    "                else:\n",
    "                    features['label'] = 'normal'\n",
    "        else:\n",
    "            features['label'] = 'unknown'\n",
    "    else:\n",
    "        features['label'] = 'unknown'\n",
    "    \n",
    "    return features\n",
    "\n",
    "def process_bgp_data():\n",
    "    \"\"\"\n",
    "    Process BGP data file and extract features with specified time window\n",
    "    \"\"\"\n",
    "    print(f\"Reading input file: {INPUT_FILE}\")\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values('Timestamp')\n",
    "    \n",
    "    # Get start and end times\n",
    "    start_time = df['Timestamp'].min()\n",
    "    end_time = df['Timestamp'].max()\n",
    "    \n",
    "    print(f\"Time range: {start_time} to {end_time}\")\n",
    "    \n",
    "    # Create time windows based on the specified frequency\n",
    "    window_freq = WINDOW_SIZE\n",
    "    \n",
    "    # Use pandas resample to create time windows\n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "    \n",
    "    # Create empty list to store features\n",
    "    all_features = []\n",
    "    \n",
    "    # Group data by time windows\n",
    "    grouped = df.groupby(pd.Grouper(freq=window_freq))\n",
    "    \n",
    "    window_count = 0\n",
    "    total_windows = len(grouped)\n",
    "    \n",
    "    # Process each window\n",
    "    for window_start, window_df in grouped:\n",
    "        if not window_df.empty:\n",
    "            window_df = window_df.reset_index()  # Reset index to get Timestamp as column\n",
    "            features = extract_features(window_df)\n",
    "            \n",
    "            if features:\n",
    "                window_end = window_start + pd.Timedelta(window_freq)\n",
    "                features['window_start'] = window_start\n",
    "                features['window_end'] = window_end\n",
    "                all_features.append(features)\n",
    "                window_count += 1\n",
    "                \n",
    "                # Print progress\n",
    "                if window_count % 50 == 0:\n",
    "                    print(f\"Processed {window_count}/{total_windows} windows ({window_count/total_windows:.1%})...\")\n",
    "    \n",
    "    print(f\"Total windows processed: {window_count}\")\n",
    "    \n",
    "    # Create features dataframe\n",
    "    if all_features:\n",
    "        features_df = pd.DataFrame(all_features)\n",
    "        \n",
    "        # Ensure all 25 required features are present\n",
    "        required_features = [\n",
    "            'dups', 'edit_distance_avg', 'edit_distance_dict_0', 'edit_distance_dict_1',\n",
    "            'nlri_ann', 'origin_0', 'origin_2', 'imp_wd', 'rare_ases_avg', 'imp_wd_spath',\n",
    "            'unique_as_path_max', 'edit_distance_dict_2', 'edit_distance_dict_4', \n",
    "            'edit_distance_dict_6', 'edit_distance_max', 'edit_distance_unique_dict_0',\n",
    "            'edit_distance_unique_dict_1', 'announcements', 'origin_changes', 'flaps',\n",
    "            'nadas', 'number_rare_ases', 'withdrawals', 'as_path_max', 'imp_wd_dpath'\n",
    "        ]\n",
    "        \n",
    "        # Check if all required features are present\n",
    "        missing_features = [feature for feature in required_features if feature not in features_df.columns]\n",
    "        if missing_features:\n",
    "            print(f\"Warning: Missing features: {missing_features}\")\n",
    "            # Add missing features with default value 0\n",
    "            for feature in missing_features:\n",
    "                features_df[feature] = 0\n",
    "        \n",
    "        # Write to output file\n",
    "        features_df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"Features written to {OUTPUT_FILE}\")\n",
    "        print(f\"Features extracted: {', '.join(features_df.columns)}\")\n",
    "        \n",
    "        return features_df\n",
    "    else:\n",
    "        print(\"No features extracted. Check input data.\")\n",
    "        return None\n",
    "\n",
    "# Main execution for Jupyter - uncomment when ready\n",
    "features_df = process_bgp_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (generator)",
   "language": "python",
   "name": "generator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
